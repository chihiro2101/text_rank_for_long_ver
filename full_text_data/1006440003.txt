Root-finding algorithms

bracketing methods . bracketing methods determine successively smaller intervals ( brackets ) that contain a root . when the interval is small enough , then a root has been found . they generally use the intermediate value theorem , which asserts that if a continuous function has values of opposite signs at the end points of an interval , then the function has at least one root in the interval . therefore , they require to start with an interval such that the function takes opposite signs at the end points of the interval . however , in the case of polynomials there are other methods ( descartes ' rule of signs , budan 's theorem and sturm 's theorem ) for getting information on the number of roots in an interval . they lead to efficient algorithms for real-root isolation of polynomials , which ensure finding all real roots with a guaranteed accuracy . bisection method . the simplest root-finding algorithm is the bisection method . let be a continuous function , for which one knows an interval such that and have opposite signs ( a bracket ) . let be the middle of the interval ( the midpoint or the point that bisects the interval ) . then either and , or and have opposite signs , and one has divided by two the size of the interval . although the bisection method is robust , it gains one and only one bit of accuracy with each iteration . other methods , under appropriate conditions , can gain accuracy faster . false position ( regula falsi ) . the false position method , also called the regula falsi method , is similar to the bisection method , but instead of using bisection search 's middle of the interval it uses the -intercept of the line that connects the plotted function values at the endpoints of the interval , that is : c \frac , that is a slower convergence than with three steps of newton 's method . finding roots in pairs . if the given polynomial only has real coefficients , one may wish to avoid computations with complex numbers . to that effect , one has to find quadratic factors for pairs of conjugate complex roots . the application of the multidimensional newton 's method to this task results in bairstow 's method . the real variant of jenkins–traub algorithm is an improvement of this method . finding all roots at once . the simple durand–kerner and the slightly more complicated aberth method simultaneously find all of the roots using only simple complex number arithmetic . accelerated algorithms for multi-point evaluation and interpolation similar to the fast fourier transform can help speed them up for large degrees of the polynomial . it is advisable to choose an asymmetric , but evenly distributed set of initial points . the implementation of this method in the free software mpsolve is a reference for its efficiency and its accuracy . another method with this style is the graeffe 's method ( sometimes also ascribed to lobachevsky ) , which uses polynomial transformations to repeatedly and implicitly square the roots . this greatly magnifies variances in the roots . applying vieta 's formulas , one obtains easy approximations for the modulus of the roots , and with some more effort , for the roots themselves . exclusion and enclosure methods . several fast tests exist that tell if a segment of the real line or a region of the complex plane contains no roots . by bounding the modulus of the roots and recursively subdividing the initial region indicated by these bounds , one can isolate small regions that may contain roots and then apply other methods to locate them exactly . all these methods involve finding the coefficients of shifted and scaled versions of the polynomial . for large degrees , fft-based accelerated methods become viable . for real roots , see next sections . the lehmer–schur algorithm uses the schur–cohn test for circles ; a variant , wilf 's global bisection algorithm uses a winding number computation for rectangular regions in the complex plane . the splitting circle method uses fft-based polynomial transformations to find large-degree factors corresponding to clusters of roots . the precision of the factorization is maximized using a newton-type iteration . this method is useful for finding the roots of polynomials of high degree to arbitrary precision ; it has almost optimal complexity in this setting . real-root isolation . finding the real roots of a polynomial with real coefficients is a problem that has received much attention since the beginning of 19th century , and is still an active domain of research . most root-finding algorithms can find some real roots , but can not certify having found all the roots . methods for finding all complex roots , such as aberth method can provide the real roots . however , because of the numerical instability of polynomials ( see wilkinson 's polynomial ) , they may need arbitrary-precision arithmetic for deciding which roots are real . moreover , they compute all complex roots when only few are real . it follows that the standard way of computing real roots is to compute first disjoint intervals , called isolating intervals , such that each one contains exactly one real root , and together they contain all the roots . this computation is called real-root isolation . having isolating interval , one may use fast numerical methods , such as newton 's method for improving the precision of the result . the oldest complete algorithm for real-root isolation results from sturm 's theorem . however , it appears to be much less efficient than the methods based on descartes ' rule of signs and vincent 's theorem . these methods divide into two main classes , one using continued fractions and the other using bisection . both method have been dramatically improved since the beginning of 21st century . with these improvements they reach a computational complexity that is similar to that of the best algorithms for computing all the roots ( even when all roots are real ) . these algorithms have been implemented and are available in mathematica ( continued fraction method ) and maple ( bisection method ) . both implementations can routinely find the real roots of polynomials of degree higher than 1,000 . finding multiple roots of polynomials . most root-finding algorithms behave badly when there are multiple roots or very close roots . however , for polynomials whose coefficients are exactly given as integers or rational numbers , there is an efficient method to factorize them into factors that have only simple roots and whose coefficients are also exactly given . this method , called square-free factorization , is based on the multiple roots of a polynomial being the roots of the greatest common divisor of the polynomial and its derivative . the square-free factorization of a polynomial p is a factorization p p_1p_2^2\cdots p_k^k where each p_i is either 1 or a polynomial without multiple roots , and two different p_i do not have any common root . an efficient method to compute this factorization is yun 's algorithm . see also . list of root finding algorithms gnu scientific library th root algorithm . references .

in mathematics and computing , a root-finding algorithm is an algorithm for finding zeroes , also called '' roots '' , of continuous functions . a zero of a function , from the real numbers to real numbers or from the complex numbers to the complex numbers , is a number such that . as , generally , the zeroes of a function can not be computed exactly nor expressed in closed form , root-finding algorithms provide approximations to zeroes , expressed either as floating point numbers or as small isolating intervals , or disks for complex roots ( an interval or disk output being equivalent to an approximate output together with an error bound ) . equation solving is the same as finding the roots of the function . thus root-finding algorithms allow solving any equation defined by continuous functions . however , most root-finding algorithms do not guarantee that they will find all the roots ; in particular , if such an algorithm does not find any root , that does not mean that no root exists . most numerical root-finding methods use iteration , producing a sequence of numbers that hopefully converge towards the root as a limit . they require one or more initial guesses of the root as starting values , then each iteration of the algorithm produces a successively more accurate approximation to the root . since the iteration must be stopped at some point these methods produce an approximation to the root , not an exact solution . many methods compute subsequent values by evaluating an auxiliary function on the preceding values . the limit is thus a fixed point of the auxiliary function , which is chosen for having the roots of the original equation as fixed points , and for converging rapidly to these fixed points . the behaviour of general root-finding algorithms is studied in numerical analysis . however , for polynomials , root-finding study belongs generally to computer algebra , since algebraic properties of polynomials are fundamental for the most efficient algorithms . the efficiency of an algorithm may depend dramatically on the characteristics of the given functions . for example , many algorithms use the derivative of the input function , while others work on every continuous function . in general , numerical algorithms are not guaranteed to find all the roots of a function , so failing to find a root does not prove that there is no root . however , for polynomials , there are specific algorithms that use algebraic properties for certifying that no root is missed , and locating the roots in separate intervals ( or disks for complex roots ) that are small enough to ensure the convergence of numerical methods ( typically newton 's method ) to the unique root so located .