in the case of normal distribution data , the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean , and 1 in 370 will deviate by three times the standard deviation in a sample of 1000 observations , the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected , being less than twice the expected number and hence within 1 standard deviation of the expected number – see poisson distribution – and not indicate an anomaly in general , if the nature of the population distribution is known a priori , it is possible to test if the number of outliers deviate significantly from what can be expected : for a given cutoff ( so samples fall beyond the cutoff with probability p ) of a given distribution , the number of outliers will follow a binomial distribution with parameter p , which can generally be well-approximated by the poisson distribution with λ pn additionally , the pathological appearance of outliers of a certain form appears in a variety of datasets , indicating that the causative mechanism for the data might differ at the extreme end ( king effect ) model-based methods which are commonly used for identification assume that the data are from a normal distribution , and identify observations which are deemed '' unlikely '' based on mean and standard deviation : chauvenet 's criterion grubbs 's test for outliers dixon 's q test astm e178 standard practice for dealing with outlying observations mahalanobis distance and leverage are often used to detect outliers , especially in the development of linear regression models it is proposed to determine in a series of m observations the limit of error , beyond which all observations involving so great an error may be rejected , provided there are as many as n such observations the principle upon which it is proposed to solve this problem is , that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many , and no more , abnormal observations the modified thompson tau test is a method used to determine if an outlier exists in a data set the strength of this method lies in the fact that it takes into account a data set 's standard deviation , average and provides a statistically determined rejection zone ; thus providing an objective method to determine if a data point is an outlier even when a normal distribution model is appropriate to the data being analyzed , outliers are expected for large sample sizes and should not automatically be discarded if that is the case rejection of outliers is more acceptable in areas of practice where the underlying model of the process being measured and the usual distribution of measurement error are confidently known exclusion can also be a consequence of the measurement process , such as when an experiment is not entirely capable of measuring such extreme values , resulting in censored data in regression problems , an alternative approach may be to only exclude points which exhibit a large degree of influence on the estimated coefficients , using a measure such as cook 's distance 