dht research was originally motivated , in part , by peer-to-peer ( p2p ) systems such as freenet , gnutella , bittorrent and napster , which took advantage of resources distributed across the internet to provide a single useful application napster , the first large-scale p2p content delivery system , required a central index server : each node , upon joining , would send a list of locally held files to the server , which would perform searches and refer the queries to the nodes that held the results gnutella and similar networks moved to a query flooding model in essence , each search would result in a message being broadcast to every other machine in the network freenet is fully distributed , but employs a heuristic key-based routing in which each file is associated with a key , and files with similar keys tend to cluster on a similar set of nodes one drawback is that , like freenet , dhts only directly support exact-match search , rather than keyword search , although freenet 's routing algorithm can be generalized to any key type where a closeness operation can be defined dhts characteristically emphasize the following properties : decentralized computing : the nodes collectively form the system without any central coordination a key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system – most commonly , o ( log n ) of the n participants ( see below ) – so that only a limited amount of work needs to be done for each change in membership finally , dhts must deal with more traditional distributed systems issues such as load balancing , data integrity , and performance ( in particular , ensuring that operations such as routing and data storage or retrieval complete quickly ) an overlay network then connects the nodes , allowing them to find the owner of any given key in the keyspace once these components are in place , a typical use of the dht for storage and retrieval might proceed as follows the keyspace partitioning and overlay network components are described below with the goal of capturing the principal ideas common to most dhts ; many designs differ in the details the two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem since any change in ownership typically corresponds to bandwidth-intensive movement of objects stored in the dht from one node to another , minimizing such reorganization is required to efficiently support high rates of churn ( node arrival and failure ) in rendezvous hashing , also called highest random weight ( hrw ) hashing , all clients use the same hash function h ( ) ( chosen ahead of time ) to associate a key to one of the n available servers many dhts use that flexibility to pick neighbors that are close in terms of latency in the physical underlying network two systems that are based on this approach are structella , which implements flooding and random walks on a pastry overlay , and dq-dht , which implements a dynamic querying search algorithm over a chord network in the real world the key could be a hash of a file 's content rather than a hash of a file 's name to provide content-addressable storage , so that renaming of the file does not prevent users from finding it for example , key could be the node and associated data could describe how to contact this node in the simplest case , is just a random number that is directly used as key ( so in a 160-bit dht will be a 160-bit number , usually randomly chosen ) in some dht designs , nodes agree to handle a certain keyspace range , the size of which may be chosen dynamically , rather than hard-coded some advanced dhts like kademlia perform iterative lookups through the dht first in order to select a set of suitable nodes and send messages only to those nodes , thus drastically reducing useless traffic , since published messages are only sent to nodes that seem suitable for storing the key ; and iterative lookups cover just a small set of nodes rather than the entire dht , reducing useless forwarding in such dhts , forwarding of messages may only occur as part of a self-healing algorithm : if a target node receives a message , but believes that is out of its handled range and a closer node ( in terms of dht keyspace ) is known , the message is forwarded to that node since on most machines sending messages is much more expensive than local hash table accesses , it makes sense to bundle many messages concerning a particular node into a single batch 